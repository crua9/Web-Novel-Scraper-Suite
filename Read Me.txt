I made this so I can enjoy stories on Royal Road and Scribblehub. One script grabs links to given chapters and puts it in a txt file. And the other script takes those links and grabs the stories and puts them in a txt file. From there I just throw the txt file in a TTS and listen to it like an audio book while I do things.

Doing it by hand it took an hour to do about 50 or so chapters. This takes seconds.

+++++++

What does the chapter scraper script do?

It reads chapter_list.txt to find URLs that haven't been scraped yet (the ones without a ✔ checkmark).
It opens a visible browser window that should not be closed or minimized.
It navigates to each URL on its to-do list, one by one.
It detects the website (ScribbleHub or Royal Road) and uses the correct method to find the chapter title and story content.
As each chapter is successfully scraped, it appends the formatted content to your final output file (e.g., My Awesome Story.txt).
It then updates chapter_list.txt, marking the line with a ✔ and the chapter title so it won't be scraped again.
If a chapter fails to load, it will automatically retry a few times before moving on.
After all scraping is finished, it re-reads the final output file and sorts all the chapters into the correct chronological order.
Any URLs that could not be scraped are saved in a separate failed_chapters.txt file for you to review.
What the Chapter Link Grabber Script Do?

It opens a visible browser window (do not close or minimize it).
It navigates to the URL you provided.
It detects the website (ScribbleHub or Royal Road) and uses the appropriate method to find all chapter links.
For ScribbleHub, it will click the "Show All Chapters" button and wait for the full list to load.
For Royal Road, it reads the chapter data directly from the page's code.
It organizes the links in chronological order.
It creates a new folder named links.
Finally, it saves the chapter URLs into one or more .txt files inside the links folder, split according to the "links per file" number you specified.
++++++++++++++

Prerequisites
Before you can run this script, you need to have Python installed on your system.

Python 3: This script is written for Python 3. You can download it from the official Python website. During installation, make sure to check the box that says "Add Python to PATH".

pip: This is Python's package installer and is usually included with Python 3.

Installation
This script relies on a powerful browser automation library called Playwright. To get it set up, you'll need to run two commands in your terminal or command prompt.

Install the Playwright library:
This command uses pip to download and install the Playwright package for Python.

Bash

pip install playwright
Install the necessary browsers:
Playwright needs its own versions of browsers to work correctly. This command downloads the browsers (like Chromium, which it uses by default) into a location where Playwright can find them. This might take a few minutes.

Bash

playwright install
Once both of these commands have completed successfully, your environment is ready.

How to Run the Script
Navigate to the folder containing the scraper.py script in your terminal or command prompt.

Run the script using the following command:

Bash

python scraper.py
The script will then prompt you for the following information:

Story URL: Paste the full URL of the main series/fiction page from either ScribbleHub or Royal Road.

Output file name: This is the base name for your output files (e.g., "World Keeper Links").

Links per file: This is the number of chapter links you want to save in each text file.

What the Script Does
It opens a visible browser window (do not close or minimize it).

It navigates to the URL you provided.

It detects the website (ScribbleHub or Royal Road) and uses the appropriate method to find all chapter links.

For ScribbleHub, it will click the "Show All Chapters" button and wait for the full list to load.

For Royal Road, it reads the chapter data directly from the page's code.

It organizes the links in chronological order.

It creates a new folder named links.

Finally, it saves the chapter URLs into one or more .txt files inside the links folder, split according to the "links per file" number you specified.

Output file name: This is the base name for your output files (e.g., "World Keeper Links").

Links per file: This is the number of chapter links you want to save in each text file.

